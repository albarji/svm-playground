{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# SVM playground!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "<img src=\"img/header.png\" style=\"width:480px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you can experiment with Support Vector Machine models over toy data. This way you will get a better understanding of the different SVM models and their parameters. The code the visualizations is contained in the following module, you can take a look if you are curious!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import svmgui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs for binary classification try to find a boundary that separates both classes with as most margin as possible. In their linear variant, this boundary is just a line (or more generally, a hyperplane)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr>\n",
    "  <td width=\"300\"><img src=\"img/linearModel.png\"></td>\n",
    " </tr>\n",
    "</table>\n",
    "<center>Plot of a linearly separable dataset and a linear SVM model over that dataset</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it yourself! Run the following cell to open an interactive plot. You can add positive class points to the canvas by clicking on the desired position, and negative class points by doing a double click.\n",
    "\n",
    "Try creating a linearly separable dataset with a positive class cluster and a negative class cluster similar to what is shown in the figure above. A linear SVM model will be trained and represented in the plot every time you add a new point. The white line is the decision hyperplane of the SVM, while the red and blue lines represent the margins.\n",
    "\n",
    "Compare your model with the one shown in the picture above. Try adding new points to one of the class clusters, both inside the cluster and surrounding the margins. When does the decision hyperplane change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmgui.svm_classification_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More things to try**: go back to the interactive plot above and add a new point on top of the decision hyperplane. A new decision hyperplane with a narrower margin will appear. The SVM model is trying to achieve better classification accuracy on the data, sacrificing margin in order to do so. However, this behavior might be undesirable if the new point is actually a noisy data point. In such settings we would be interested in preserving a large margin, even if we misclassify such point. Such preferences can be injected into the SVM by tuning its $C$ parameter.\n",
    "\n",
    "Try now changing the value of the $C$ parameter in the control bar to see how the SVM model changes. What effect can you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr>\n",
    "  <td width=\"300\"><img src=\"img/nonlinearDataset.png\"></td>\n",
    "  <td width=\"300\"><img src=\"img/nonlinearModel.png\"></td>\n",
    " </tr>\n",
    "</table>\n",
    "<center>Plot of a linear SVM and a gaussian kernel SVM over a non-linear dataset.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the inclusion of a kernel function the SVM is able to produce solutions to **non-linear datasets** as well. To test this, we will build a new dataset on which linear models can only achieve poor results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to create a new interactive plot, and draw a dataset similar to the one shown in the picture above. You will see how the SVM can't find a linear boundary that solves the classification problem... because there is none to be found!\n",
    "\n",
    "Now select the **Gaussian** kernel in the right-hand side of the control bar. The model will change to a non-linear gaussian (RBF) kernel SVM. Does the boundary look more appropriate now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmgui.svm_classification_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK... but what about the $C$ and $\\gamma$ parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning SVM parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr>\n",
    "  <td><img src=\"img/underfitting.png\"></td>\n",
    "  <td><img src=\"img/goodfitting.png\"></td>\n",
    "  <td><img src=\"img/overfitting.png\"></td>\n",
    " </tr>\n",
    "</table>\n",
    "<center>Plots of three gaussian SVM models over a non-linear dataset with noise, for varying values of the $C$ parameter. The left plot presents a case of underfitting, while the right plot shows overfitting. A model with a correct value of $C$ is shown in the center plot.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how using a non-linear SVM effectively allows to deal with non-linear problems. However, better accuracy results can be obtained by fine-tuning the penalty parameter $C$ and the kernel parameters. This tuning, however, has to be done carefully to avoid underfitting or overfitting effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a new interactive plot in the cell below, and create a dataset similar to the previous exercise but including some blue points in the external ring to simulate noisy data.\n",
    "\n",
    "Now use the slider in the plot to train a model with a small $C$ value and observe what happens with the decision frontier. Do the same for a large $C$ value. Which of these cases corresponds to overfitting and which to underfitting? Why is this happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "svmgui.svm_classification_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that:\n",
    "\n",
    "* For very small values of $C$ the margin is much more important than the classification errors. In such settings the SVM training might produce a model that classifies all data points as belonging to the most frequent class, thus trading the misclassification of an entire class in favor of an extremely simple model.\n",
    "* For large values of $C$ the margin is completely neglected during SVM training, thus resulting in very complex models that capture all the details of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving now to the kernel parameters, since we are using the gaussian (RBF) kernel, there is a single parameter $\\gamma$ to tune, which corresponds to the gaussian width, and which you can also modify through the control bar. Go back to the interactive plot above and select a $C$ value that results in an adequate fit, neither underfitting nor overfitting. Then modify the kernel parameter $\\gamma$, observing the results for small and large values. When does overfitting/underfitting appears?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr>\n",
    "  <td width=300><img src=\"img/linearRegModel.png\"></td>\n",
    " </tr>\n",
    "</table>\n",
    "<center>Plot of a linear regression problem and linear SVR model over it</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play now with SVM for **regression** models!\n",
    "\n",
    "Run the cell above to open the interactive plot, then draw a dataset roughly following a straight line, such as the one shown in the figure above. Note that in this plot we are solving a regression problem, so the Y axis in the plot represents the target regression value. Likewise you can't add blue points as in regression problems there is no definition of \"class\".\n",
    "\n",
    "Every time you add a new point the SVR model will be updated. The learned regression line is shown as a solid line, and the $\\epsilon$-tube is marked through two dashed lines. Do you obtain a model similar to the one shown above?\n",
    "\n",
    "Note also the SVR model has an additional parameter on top of the usual $C$ parameter: $\\epsilon$. This parameter controls the width of the regression tube, and also sets the amount of acceptable regression error in our model. Try modifying $\\epsilon$ to some other values. How do the angle of the regression line and the margins change? How can you explain this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmgui.svm_regression_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for very large values of $\\epsilon$ all the data points fall into the $\\epsilon$-tube, and thus the SVR considers that no significant regression errors are being made. Therefore, since the SVR also tries to minimize the complexity of the model, the output regression will tend to be the simplest of all: predict a constant value for any point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr>\n",
    "  <td><img src=\"img/reg_underfitting.png\"></td>\n",
    "  <td><img src=\"img/reg_goodfitting.png\"></td>\n",
    "  <td><img src=\"img/reg_overfitting.png\"></td>\n",
    " </tr>\n",
    "</table>\n",
    "<center>Different SVR models trained over a non-linear dataset, for varying values of the model parameters. The left-hand side image shows an underfitting case, while the right-hand side image presents an overfitting case. The image at the middle shows the results of a correct tuning of parameters.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now address a non-linear regression problem with SVR. Use the cell below to draw a sinusoid-like dataset, such as the one shown above. Now select the Gaussian kernel. Are you able to obtain a well fitted model?\n",
    "\n",
    "After that, try to do some tuning. A non-linear SVR model has three parameters to tune: the $C$ penalty parameter, the kernel parameter $\\gamma$ and the tube width $\\epsilon$. Note the influence of $\\epsilon$ on the model is more significant in non-linear problems, as non-linear models are more prone to overfitting due to their higher capacity, i.e. their ability to represent very complex functions. Play with the parameters and check when your model underfits or overfits, and which values produce the better looking model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmgui.svm_regression_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-class SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see now another SVM model, addressing a different kind of problem: One-class SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr>\n",
    "  <td><img src=\"img/oneclass_underfitting.png\"></td>\n",
    "  <td><img src=\"img/oneclass_goodfitting.png\"></td>\n",
    "  <td><img src=\"img/oneclass_overfitting.png\"></td>\n",
    " </tr>\n",
    "</table>\n",
    "<center>Different One-class SVM models trained over a non-linear dataset, for varying values of $\\nu$ that produce different supports</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to open an interactive plot for one-class SVM. Draw a dataset following a ball-like shape, similar to the one shown above, with just points from a single class. Every time you add a point to the plot a one-class SVM with gaussian kernel will be trained, showing the decision boundary.\n",
    "\n",
    "Note One-class SVMs do not have a $C$ parameter, but a $\\nu$ parameter instead. When leaving $\\nu$ and $\\gamma$ unchanged you should obtain a model similar to the one shown in the center image above. The solid line represents the decision boundary of the One-class SVM: any data point outside this enclosing is regarded as an outlier or anomaly by the model. The larger the area enclosed inside this line - also known as support of the distribution - the less sensitive the model will be to outliers.\n",
    "\n",
    "The effect of $\\nu$ becomes clear when building the One-class SVM with two extreme options of this parameter. Try modifying the value $\\nu$ and observe the changes in the model. How does the support change? For which values do you obtain models similar to those shown above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmgui.svm_oneclass_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note $\\nu$ can be shown to be an upper bound on the fraction of training errors (data not enclosed by the model), as well as a lower bound in the fraction of support vectors. Because of this, the model tries to produce a larger support the smaller $\\nu$ is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to the interactive plot above and add some points to the dataset, far away from the original data. These points are anomalies that do not follow the usual data distribution. Find a value for $\\nu$ so that the model captures most of the points in the central distribution but none of the anomalies. Which $\\nu$ value provides the best solution?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
