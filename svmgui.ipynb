{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Lab assignment: visualizing the SVM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "<img src=\"img/header.png\" style=\"width:480px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "In this assignment we will experiment with the SVM model by applying it to toy problems, analyzing in a visual way the effects of the different SVM parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "Throughout this notebook you will find empty cells that you will need to fill with your own code. Follow the instructions in the notebook and pay special attention to the following symbols.\n",
    "\n",
    "<table>\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "You will be presented with inquiries about the work you have developed so far, or asked to perform a particular task and ponder the results obtained. You do not need to write the answers or hand them in, but try to consider possible answers and come to a conclusion.</td></tr>\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">This is a hint or useful observation that can help you solve this assignment. You are not expected to write any solution, but you should pay attention to them to understand the assignment.</td></tr>\n",
    " <tr><td width=\"80\"><img src=\"img/pro.png\" style=\"width:auto;height:auto\"><td style=\"text-align:left\">This is an advanced and voluntary exercise that can help you gain a deeper knowledge into the topic. Good luck!</td></tr>\n",
    "</table>\n",
    "\n",
    "To avoid missing packages and compatibility issues you should run this notebook under one of the [recommended SVM environment files](https://github.com/albarji/teaching-environments/tree/master/SVMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, if you need any help on the usage of a Python function you can place the writing cursor over its name and press Shift+Tab to produce a pop-out with related documentation. This will only work inside code cells. \n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr>\n",
    "  <td width=\"300\"><img src=\"img/linearModel.png\"></td>\n",
    " </tr>\n",
    "</table>\n",
    "<center>Plot of a linearly separable dataset and a linear SVM model over that dataset</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by studying linear Support Vector Machine models. To do so, we will make use of an interactive plot  contained in the provided svmgui.py module. You can take a look at the source code if you are curious!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import svmgui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "Run the cell below to open an interative plot, and create a linearly separable dataset with a positive class cluster and a negative class cluster similar to what is shown in the figure above. You can add positive class points to the canvas by clicking on the desired position, and negative class points by doing a double click.<br>\n",
    "<br>     \n",
    "A linear SVM model will be trained and represented in the plot every time you add a new point. The white line is the decision hyperplane of the SVM, while the red and blue lines represent the margins.<br>\n",
    "<br>\n",
    "Compare your model with the one shown in the picture above. Try adding new points to one of the class clusters, both inside the cluster and surrounding the margins. When does the decision hyperplane change?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmgui.svm_classification_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "Go back to the interactive plot above and add a new point on top of the decision hyperplane. A new decision hyperplane with a narrower margin will appear. The SVM model is trying to achieve better classification accuracy on the data, sacrificing margin in order to do so. However, this behavior might be undesirable if the new point is actually a noisy data point. In such settings we would be interested in preserving a large margin, even if we misclassify such point. Such preferences can be injected into the SVM by tuning its $C$ parameter.<br>\n",
    "     <br>\n",
    "Try now changing the value of the $C$ parameter in the control bar to see how the SVM model changes. What effect can you observe?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr>\n",
    "  <td width=\"300\"><img src=\"img/nonlinearDataset.png\"></td>\n",
    "  <td width=\"300\"><img src=\"img/nonlinearModel.png\"></td>\n",
    " </tr>\n",
    "</table>\n",
    "<center>Plot a linear SVM and a gaussian kernel SVM over a non-linear dataset.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the inclusion of kernel function the SVM is able to produce solutions to non-linear datasets as well. To test this, we will build a new dataset on which linear models can only achieve poor results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "     Run the cell below to create a new interactive plot, and draw a dataset similar to the one shown in the picture above. Do you think the results are good? <br>\n",
    "     <br>\n",
    "     Now select the <b>Gaussian</b> kernel in the right-hand side of the control bar. The model will change to a non-linear gaussian (RBF) kernel SVM. Does the separating frontier look more appropriate now?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmgui.svm_classification_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning SVM parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr>\n",
    "  <td><img src=\"img/underfitting.png\"></td>\n",
    "  <td><img src=\"img/goodfitting.png\"></td>\n",
    "  <td><img src=\"img/overfitting.png\"></td>\n",
    " </tr>\n",
    "</table>\n",
    "<center>Plots of three gaussian SVM models over a non-linear dataset with noise, for varying values of the $C$ parameter. The left plot presents a case of underfitting, while the right plot shows overfitting. A model with a correct value of $C$ is shown in the center plot.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how using a non-linear SVM effectively allows to deal with non-linear problems. However, better accuracy results can be obtained by fine-tuning the penalty parameter $C$ and the kernel parameters. This tuning, however, has to be done carefully to avoid underfitting or overfitting effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "Open a new interactive plot in the cell below, and create a dataset similar to the previous exercise but including some blue points in the external ring to simulate noisy data.<br>\n",
    "<br>     \n",
    "Now use the slider in the plot to train a model with a small $C$ value and observe what happens with the decision frontier. Do the same for a large $C$ value. Which of these cases corresponds to overfitting and which to underfitting? Why is this happening?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmgui.svm_classification_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    " \n",
    "For very small values of $C$ the margin is much more important than the classification errors. In such settings the SVM training might produce a model that classifies all data points as belonging to the most frequent class, thus trading the misclassification of an entire class in favor of an extremely simple model.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "For large values of $C$ the margin is completely neglected during SVM training, thus resulting in very complex models that capture all the details of the training data.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving now to the kernel parameters, since we are using the gaussian (RBF) kernel, there is a single parameter $\\gamma$ to tune, which corresponds to the gaussian width, and which you can also modify through the control bar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "Go back to the interactive plot above and select a $C$ value that results in an adequate fit, neither underfitting nor overfitting. Then modify the kernel parameter $\\gamma$, observing the results for small and large values. When does overfitting/underfitting appears?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr>\n",
    "  <td width=300><img src=\"img/linearRegModel.png\"></td>\n",
    " </tr>\n",
    "</table>\n",
    "<center>Plot of a linear regression problem and linear SVR model over it</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now illustrate the use of Support Vector Machine for regression using a new interactive plot. We will start with the linear support vector regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "Run the cell above to open the interactive plot, then draw a dataset roughly following a straight line, such as the one shown in the figure above. Note that in this plot we are solving a regression problem, so the Y axis in the plot represents the target regression value. Likewise you can't add blue points as in regression problems there is no definition of \"class\".<br>\n",
    " <br>\n",
    "Every time you add a new point the SVR model will be updated. The learned regression line is shown as a solid line, and the $\\epsilon$-tube is marked through two dashed lines. Do you obtain a model similar to the one shown above?<br>\n",
    "<br>\n",
    "Note also the SVR model has an additional parameter on top of the usual $C$ parameter: $\\epsilon$. This parameter controls the width of the regression tube, and also sets the amount of acceptable regression error in our model. Try modifying $\\epsilon$ to some other values. How do the angle of the regression line and the margins change? How can you explain this?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmgui.svm_regression_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "For very large values of $\\epsilon$ all the data points fall into the $\\epsilon$-tube, and thus the SVR considers that no significant regression errors are being made. Therefore, since SVR also tries to minimize the complexity of the model, the output regression will tend to be the simplest of all: predict a constant value for any point.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr>\n",
    "  <td><img src=\"img/reg_underfitting.png\"></td>\n",
    "  <td><img src=\"img/reg_goodfitting.png\"></td>\n",
    "  <td><img src=\"img/reg_overfitting.png\"></td>\n",
    " </tr>\n",
    "</table>\n",
    "<center>Different SVR models trained over a non-linear dataset, for varying values of the model parameters. The left-hand side image shows an underfitting case, while the right-hand side image presents an overfitting case. The image at the middle shows the results of a correct tuning of parameters.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now address a non-linear regression problem with SVR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "Use the cell below to draw a sinusoid-like dataset, such as the one shown above. Now select the Gaussian kernel. Are you able to obtain a well fitted model? <br>\n",
    "<br>\n",
    "Now try to do some tuning. A non-linear SVR model has three parameters to tune: the $C$ penalty parameter, the kernel parameter $\\gamma$ and the tube width $\\epsilon$. Note the influence of $\\epsilon$ on the model is more significant in non-linear problems, as non-linear models are more prone to overfitting due to their higher capacity, i.e. their ability to represent very complex functions. Play with the parameters and check when your model underfits or overfits, and which values produce the better looking model.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmgui.svm_regression_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-class SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see now another SVM model, addressing a different kind of problem: One-class SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr>\n",
    "  <td><img src=\"img/oneclass_underfitting.png\"></td>\n",
    "  <td><img src=\"img/oneclass_goodfitting.png\"></td>\n",
    "  <td><img src=\"img/oneclass_overfitting.png\"></td>\n",
    " </tr>\n",
    "</table>\n",
    "<center>Different One-class SVM models trained over a non-linear dataset, for varying values of $\\nu$ that produce different supports</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "Run the cell below to open an interactive plot for one-class SVM. Draw a dataset following ball-like shape, similar to the one shown above, with just points from a single class. Every time you add a point to the plot a one-class SVM with gaussian kernel will be trained, showing the decision boundary.<br>\n",
    "<br>\n",
    "Note One-class SVMs do not have a $C$ parameter, but a $\\nu$ parameter instead. When leaving $\\nu$ and $\\gamma$ unchanged you should obtain a model similar to the one shown in the center image above. The solid line represents the decision boundary of the One-class SVM: any data point outside this enclosing is regarded as an outlier or anomaly by the model. The larger the area enclosed inside this line - also known as support of the distribution - the less sensitive the model will be to outliers.<br>\n",
    "<br>\n",
    "The effect of $\\nu$ becomes clear when building the One-class SVM with two extreme options of this parameter. Try modifying the value $\\nu$ and observe the changes in the model. How does the support change? For which values do you obtain models similar to those shown above?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmgui.svm_oneclass_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "$\\nu$ can be shown to be an upper bound on the fraction of training errors (data not enclosed by the model), as well as a lower bound in the fraction of support vectors. Because of this, the model tries to produce a larger support the smaller $\\nu$ is.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td style=\"text-align:left\">\n",
    "Go back to the interactive plot above and add some points to the dataset, far away from the original data. These points are anomalies that do not follow the usual data distribution. Find a value for $\\nu$ so that the model captures most of the points in the central distribution but none of the anomalies. Which $\\nu$ value provides the best solution?\n",
    " </td></tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
